{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount to Google Drive"
      ],
      "metadata": {
        "id": "R58eBQzCuUeE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXUIpUjCpKxS",
        "outputId": "619c2110-8fc0-4261-df73-b1e4ffd7f9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "#images_path = \"/content/drive/MyDrive/Traffic Sign Detection/ALL DATA\"\n",
        "images_path = \"/data/ALL Data\"\n",
        "import sys\n",
        "#sys.path.append('/content/drive/MyDrive/Traffic Sign Detection/source')\n",
        "sys.path.append('/source')\n",
        "import models as md\n",
        "import utils\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation\n",
        "Below code is to resize images and create test and training folders. No need to run them again."
      ],
      "metadata": {
        "id": "Gf44D94C1i6v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HtfwnnEJe72"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Target size for every image\n",
        "target_size = (500, 500)\n",
        "os.makedirs(\"data/resized500\", exist_ok=True)\n",
        "\n",
        "# Iterating through class_0 images to resize\n",
        "for image_file in os.listdir(\"{}/class_0\".format(images_path)):\n",
        "    image_path = os.path.join(\"{}/class_0\".format(images_path), image_file)\n",
        "    img = Image.open(image_path)\n",
        "    img_resized = img.resize(target_size)\n",
        "\n",
        "    #kaydet\n",
        "    output_path = os.path.join(\"data/resized500\", image_file)\n",
        "    img_resized.save(output_path)\n",
        "\n",
        "# Iterating through class_1 images to resize\n",
        "for image_file in os.listdir(\"{}/class_1\".format(images_path)):\n",
        "    image_path = os.path.join(\"{}/class_1\".format(images_path), image_file)\n",
        "    img = Image.open(image_path)\n",
        "    img_resized = img.resize(target_size)\n",
        "\n",
        "    #kaydet\n",
        "    output_path = os.path.join(\"data/resized500\", image_file)\n",
        "    img_resized.save(output_path)\n",
        "\n",
        "print(\"Image resizing complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD8P7deA4G-R"
      },
      "source": [
        "# Splitting Data Set Into Training and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHNZ6yEH6ilH"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO_aEzlC1SPj"
      },
      "outputs": [],
      "source": [
        "images_path = \"/content/drive/MyDrive/Traffic Sign Detection/ALL DATA\"\n",
        "\n",
        "# Path to store training and test datasets\n",
        "destination_folder = \"/content/drive/MyDrive/Traffic Sign Detection/cnn1/training and test\"\n",
        "\n",
        "# Class names\n",
        "classes = [\"class_0\", \"class_1\"]\n",
        "\n",
        "# Create train and test folders\n",
        "train_folder = os.path.join(destination_folder, \"train\")\n",
        "test_folder = os.path.join(destination_folder, \"test\")\n",
        "\n",
        "os.makedirs(train_folder, exist_ok=True)\n",
        "os.makedirs(test_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOpR3kfGrDf3"
      },
      "outputs": [],
      "source": [
        "#copy images from original dataset to train and test folders\n",
        "for class_name in classes:\n",
        "    class_folder = os.path.join(images_path, class_name)\n",
        "\n",
        "    # Create class folders in train and test folders\n",
        "    train_class_folder = os.path.join(train_folder, class_name)\n",
        "    test_class_folder = os.path.join(test_folder, class_name)\n",
        "\n",
        "    os.makedirs(train_class_folder, exist_ok=True)\n",
        "    os.makedirs(test_class_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "    # Split images into train and test sets\n",
        "    image_list = os.listdir(class_folder)\n",
        "    train_images, test_images = train_test_split(image_list, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Copy images to train folder\n",
        "    for img in train_images:\n",
        "        source_path = os.path.join(class_folder, img)\n",
        "        destination_path = os.path.join(train_class_folder, img)\n",
        "        shutil.copy(source_path, destination_path)\n",
        "\n",
        "    # Copy images to test folder\n",
        "    for img in test_images:\n",
        "        source_path = os.path.join(class_folder, img)\n",
        "        destination_path = os.path.join(test_class_folder, img)\n",
        "        shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(\"Dataset split into train and test folders.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZihlvSld4MjY"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOsOjnkJ6g5s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49sSHt1I3OHo",
        "outputId": "5139c2a5-43e9-4374-cde1-8d8f64ad6294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 852 images belonging to 2 classes.\n",
            "Found 272 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Paths to store splitted datasets\n",
        "train_data_dir = \"/data/cnn1/training and test/train\"\n",
        "test_data_dir = \"/data/cnn1/training and test/test\"\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation and normalization\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,      # Normalize pixel values to the range [0, 1]\n",
        "    shear_range=0.2,     # Shear transformations\n",
        "    zoom_range=0.2,      # Zoom transformations\n",
        "    horizontal_flip=True  # Horizontal flips\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create train generators with established batch size\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(500, 500),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\"  # Assumes two classes\n",
        ")\n",
        "\n",
        "# Create validation generators with established batch size\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(500, 500),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB9AB3Tq6_K9",
        "outputId": "32f9133d-e8f3-4d42-fb9a-a2f556b8b219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.7073\n",
            "Epoch 1: val_accuracy improved from -inf to 0.71875, saving model to best_tsmodel1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r26/26 [==============================] - 121s 4s/step - loss: 0.7008 - accuracy: 0.7073 - val_loss: 0.4808 - val_accuracy: 0.7188\n",
            "Epoch 2/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.4325 - accuracy: 0.7841\n",
            "Epoch 2: val_accuracy improved from 0.71875 to 0.84375, saving model to best_tsmodel1.h5\n",
            "26/26 [==============================] - 119s 5s/step - loss: 0.4325 - accuracy: 0.7841 - val_loss: 0.3775 - val_accuracy: 0.8438\n",
            "Epoch 3/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.4059 - accuracy: 0.8134\n",
            "Epoch 3: val_accuracy did not improve from 0.84375\n",
            "26/26 [==============================] - 92s 4s/step - loss: 0.4059 - accuracy: 0.8134 - val_loss: 0.4670 - val_accuracy: 0.7422\n",
            "Epoch 4/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.3687 - accuracy: 0.8244\n",
            "Epoch 4: val_accuracy improved from 0.84375 to 0.85938, saving model to best_tsmodel1.h5\n",
            "26/26 [==============================] - 119s 5s/step - loss: 0.3687 - accuracy: 0.8244 - val_loss: 0.3311 - val_accuracy: 0.8594\n",
            "Epoch 5/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.8524\n",
            "Epoch 5: val_accuracy improved from 0.85938 to 0.88281, saving model to best_tsmodel1.h5\n",
            "26/26 [==============================] - 136s 5s/step - loss: 0.3330 - accuracy: 0.8524 - val_loss: 0.3023 - val_accuracy: 0.8828\n",
            "Epoch 6/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.3305 - accuracy: 0.8476\n",
            "Epoch 6: val_accuracy did not improve from 0.88281\n",
            "26/26 [==============================] - 88s 3s/step - loss: 0.3305 - accuracy: 0.8476 - val_loss: 0.4067 - val_accuracy: 0.8125\n",
            "Epoch 7/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.8720\n",
            "Epoch 7: val_accuracy did not improve from 0.88281\n",
            "26/26 [==============================] - 83s 3s/step - loss: 0.2975 - accuracy: 0.8720 - val_loss: 0.2904 - val_accuracy: 0.8828\n",
            "Epoch 8/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.2802 - accuracy: 0.8878\n",
            "Epoch 8: val_accuracy did not improve from 0.88281\n",
            "26/26 [==============================] - 82s 3s/step - loss: 0.2802 - accuracy: 0.8878 - val_loss: 0.3375 - val_accuracy: 0.8438\n",
            "Epoch 9/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.9038\n",
            "Epoch 9: val_accuracy improved from 0.88281 to 0.89062, saving model to best_tsmodel1.h5\n",
            "26/26 [==============================] - 111s 4s/step - loss: 0.2432 - accuracy: 0.9038 - val_loss: 0.2527 - val_accuracy: 0.8906\n",
            "Epoch 10/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.2764 - accuracy: 0.8854\n",
            "Epoch 10: val_accuracy did not improve from 0.89062\n",
            "26/26 [==============================] - 91s 3s/step - loss: 0.2764 - accuracy: 0.8854 - val_loss: 0.2935 - val_accuracy: 0.8672\n",
            "Epoch 11/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.9049\n",
            "Epoch 11: val_accuracy improved from 0.89062 to 0.91406, saving model to best_tsmodel1.h5\n",
            "26/26 [==============================] - 125s 5s/step - loss: 0.2419 - accuracy: 0.9049 - val_loss: 0.2201 - val_accuracy: 0.9141\n",
            "Epoch 12/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.2238 - accuracy: 0.9073\n",
            "Epoch 12: val_accuracy improved from 0.91406 to 0.92578, saving model to best_tsmodel1.h5\n",
            "26/26 [==============================] - 136s 5s/step - loss: 0.2238 - accuracy: 0.9073 - val_loss: 0.1955 - val_accuracy: 0.9258\n",
            "Epoch 13/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.9049\n",
            "Epoch 13: val_accuracy did not improve from 0.92578\n",
            "26/26 [==============================] - 82s 3s/step - loss: 0.2276 - accuracy: 0.9049 - val_loss: 0.2220 - val_accuracy: 0.9180\n",
            "Epoch 14/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.2044 - accuracy: 0.9232\n",
            "Epoch 14: val_accuracy did not improve from 0.92578\n",
            "26/26 [==============================] - 89s 3s/step - loss: 0.2044 - accuracy: 0.9232 - val_loss: 0.2395 - val_accuracy: 0.8984\n",
            "Epoch 15/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9268\n",
            "Epoch 15: val_accuracy did not improve from 0.92578\n",
            "26/26 [==============================] - 89s 3s/step - loss: 0.1873 - accuracy: 0.9268 - val_loss: 0.3062 - val_accuracy: 0.8594\n",
            "Epoch 16/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.1957 - accuracy: 0.9195\n",
            "Epoch 16: val_accuracy improved from 0.92578 to 0.92969, saving model to best_tsmodel1.h5\n",
            "26/26 [==============================] - 145s 6s/step - loss: 0.1957 - accuracy: 0.9195 - val_loss: 0.1897 - val_accuracy: 0.9297\n",
            "Epoch 17/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.1605 - accuracy: 0.9390\n",
            "Epoch 17: val_accuracy improved from 0.92969 to 0.94141, saving model to best_tsmodel1.h5\n",
            "26/26 [==============================] - 120s 5s/step - loss: 0.1605 - accuracy: 0.9390 - val_loss: 0.1735 - val_accuracy: 0.9414\n",
            "Epoch 18/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9415\n",
            "Epoch 18: val_accuracy did not improve from 0.94141\n",
            "26/26 [==============================] - 92s 4s/step - loss: 0.1596 - accuracy: 0.9415 - val_loss: 0.1804 - val_accuracy: 0.9336\n",
            "Epoch 19/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9329\n",
            "Epoch 19: val_accuracy did not improve from 0.94141\n",
            "26/26 [==============================] - 92s 4s/step - loss: 0.1548 - accuracy: 0.9329 - val_loss: 0.1855 - val_accuracy: 0.9336\n",
            "Epoch 20/20\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.1449 - accuracy: 0.9476\n",
            "Epoch 20: val_accuracy did not improve from 0.94141\n",
            "26/26 [==============================] - 94s 4s/step - loss: 0.1449 - accuracy: 0.9476 - val_loss: 0.2143 - val_accuracy: 0.9023\n"
          ]
        }
      ],
      "source": [
        "model = md.firstcnn()\n",
        "checkpoint = ModelCheckpoint(\"best_tsmodel1.h5\", monitor=\"val_accuracy\", save_best_only=True, mode=\"max\", verbose=1)\n",
        "# Training the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size,\n",
        "    callbacks = [checkpoint]\n",
        ")\n",
        "\n",
        "# Saving the model\n",
        "model.save(\"/savedmodels/best_tsmodel1.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP3eZSYn12S8"
      },
      "outputs": [],
      "source": [
        "#loading the model and seeing results for the whole dataset\n",
        "model = tf.keras.models.load_model(\"/savedmodels/best_tsmodel1.h5\")\n",
        "\n",
        "confusion = [[0, 0], [0, 0]]\n",
        "\n",
        "# CLASS_0\n",
        "counter = [0, 0]\n",
        "path = \"/data/ALL DATA/class_0\"\n",
        "\n",
        "for filename in os.listdir(path):\n",
        "    #loading and preprocessing the test image\n",
        "    img_path = os.path.join(path, filename)\n",
        "    img = load_img(img_path, target_size=(500, 500))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0\n",
        "\n",
        "    #prediction\n",
        "    prediction = model.predict(img_array)\n",
        "\n",
        "    #printing the predicted class (0 or 1)\n",
        "    print(f\"Image: {filename}, Prediction: {int(round(prediction[0][0]))}\")\n",
        "    if int(round(prediction[0][0])) == 0:\n",
        "      counter[0] += 1\n",
        "    elif int(round(prediction[0][0])) == 1:\n",
        "      counter[1] += 1\n",
        "\n",
        "confusion[1][1], confusion[0][1] = counter[0], counter[1]\n",
        "\n",
        "# CLASS_1\n",
        "counter = [0, 0]\n",
        "path = \"/data/ALL DATA/class_1\"\n",
        "for filename in os.listdir(path):\n",
        "    #loading and preprocessing the test image\n",
        "    img_path = os.path.join(path, filename)\n",
        "    img = load_img(img_path, target_size=(500, 500))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0  #normalize pixel values to the range [0, 1]\n",
        "\n",
        "    #prediction\n",
        "    prediction = model.predict(img_array)\n",
        "\n",
        "    #printing the predicted class (0 or 1)\n",
        "    print(f\"Image: {filename}, Prediction: {int(round(prediction[0][0]))}\")\n",
        "    if int(round(prediction[0][0])) == 0:\n",
        "      counter[0] += 1\n",
        "    elif int(round(prediction[0][0])) == 1:\n",
        "      counter[1] += 1\n",
        "print(counter)\n",
        "\n",
        "\n",
        "confusion[0][0], confusion[1][0] = counter[1], counter[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th3IstAIYt4E"
      },
      "outputs": [],
      "source": [
        "# Printing the confusion matrix\n",
        "for i in confusion:\n",
        "  print(i)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}